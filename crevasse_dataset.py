# -*- coding: utf-8 -*-
"""crevasse_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V3cCxgB7JLezYvoWjLZdH-oCP_3eah8k
"""

from typing import Tuple, Dict

import tensorflow as tf
# import tensorflow_datasets as tfds
# from tensorflow_datasets.core import DatasetInfo

# tfds.disable_progress_bar()


!pip install git+https://github.com/naureen-nura/crevasse_detection.git
import crevasse_detection
from crevasse_detection.gris import train2, valid2, test2

#Training data
training_dr = "https://github.com/naureen-nura/crevasse_detection/tree/main/gris/train2"
#Validation data
valid_dr = "https://github.com/naureen-nura/crevasse_detection/tree/main/gris/valid2"
#Test data
test_dr = "https://github.com/naureen-nura/crevasse_detection/tree/main/gris/test2"

IMAGE_SIZE = (500, 500)
channels = 1
classes = 3


# def normalize(input_image, input_mask):
#     input_image = tf.cast(input_image, tf.float32) / 255.0
#     input_mask -= 1
#     return input_image, input_mask


def load_image_train(datapoint):
    input_image = tf.image.resize(datapoint['Images'], IMAGE_SIZE)
    input_mask = tf.image.resize(datapoint['Labels'], IMAGE_SIZE)

    if tf.random.uniform(()) > 0.5:
        input_image = tf.image.flip_left_right(input_image)
        input_mask = tf.image.flip_left_right(input_mask)

    # input_image, input_mask = normalize(input_image, input_mask)

    return input_image, input_mask


def load_image_test(datapoint):
    input_image = tf.image.resize(datapoint['Images'], IMAGE_SIZE)
    input_mask = tf.image.resize(datapoint['Labels'], IMAGE_SIZE)

    # input_image, input_mask = normalize(input_image, input_mask)

    return input_image, input_mask


def load_image_valid(datapoint):
    input_image = tf.image.resize(datapoint['Images'], IMAGE_SIZE)
    input_mask = tf.image.resize(datapoint['Labels'], IMAGE_SIZE)

    # input_image, input_mask = normalize(input_image, input_mask)

    return input_image, input_mask


# def load_data(buffer_size=1000, **kwargs) -> Tuple[tf.data.Dataset, tf.data.Dataset]:
    # dataset, info = _load_without_checksum_verification(**kwargs)
def load_data(buffer_size=1000, **kwargs):
    train = training_dr.map(load_image_train)
    test = test_dr.map(load_image_test)
    valid = valid_dr.map(load_image_valid)
    # train_dataset = train.cache().shuffle(buffer_size).take(info.splits["train"].num_examples)
    return train, test, valid


# def _load_without_checksum_verification(**kwargs) -> Tuple[Dict, DatasetInfo]:
#     builder = tfds.builder('oxford_iiit_pet:3.2.0')
#     # by setting register_checksums as True to pass the check
#     config = tfds.download.DownloadConfig(register_checksums=True)
#     builder.download_and_prepare(download_config=config)
#     dataset = builder.as_dataset()

#     return dataset, (builder.info)